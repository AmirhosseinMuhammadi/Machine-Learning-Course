{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ae70fb-1c17-4edb-94a7-70088dec451f",
   "metadata": {},
   "source": [
    "# Chapter 3: Linearity\n",
    "Linearity is a foundational concept in machine learning, characterizing models where the output is a direct, weighted sum of the input features. While simple and highly interpretable, linear models like Linear and Logistic Regression make strong assumptions about the underlying data structure. Their performance is often limited when faced with complex, non-linear relationships. However, the principle of linearity remains profoundly influential, serving as the essential building block within sophisticated non-linear architectures such as neural networks and kernel methods. This chapter covers fundamental concepts through building end-to-end projects based on the machine learning life cycle.\n",
    "\n",
    "## Table of Contents\n",
    "- [Gradient Descent](#gradient-descent)\n",
    "- Stochastic Gradient Descent\n",
    "- Linear Regression\n",
    "- Multiple Linear Regression\n",
    "- Linear Classification (Logistic Regression)\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139a763-10ba-4743-a9d6-dfcd8073067d",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "**Optimization** forms the computational core of machine learning, framing model training as the search for parameters that **minimize a loss function**. Gradient descent is the foundational **iterative algorithm** that solves this problem. By calculating the gradient of the loss, it navigates the parameter space, taking steps proportional to the negative gradient to converge toward a **local minimum**. This simple yet powerful principle of following the path of **steepest descent** enables the learning process in models ranging from **simple linear regressions** to the most **complex deep neural networks**.\n",
    "\n",
    "### Problem Statement\n",
    "The solution to the general optimization problem is given by the following equation.\n",
    "$$ w^* = \\underset{w \\in R^n}{\\arg\\min} \\, L(w)$$\n",
    "\n",
    "Where:\n",
    "- $w = (w_0, w_1, …, w_n)$ is the **decision variable** vector.\n",
    "- $L$ is the **objective/cost function** (or loss function in machine learning problems).\n",
    "\n",
    "The following table provides a summary of common loss functions and their applications in machine learning.\n",
    "### Summary of Common Loss Functions\n",
    "| Loss Function | Formula | Type | Use Cases | Key Properties |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| **L1 Loss (MAE)** | `∑\\|y - ŷ\\|` | Regression | Robust regression, outliers present | Robust to outliers, non-differentiable at 0 |\n",
    "| **L2 Loss (MSE)** | `∑(y - ŷ)²` | Regression | Standard regression, smooth outputs | Sensitive to outliers, differentiable |\n",
    "| **RMSE** | `√(∑(y - ŷ)²/n)` | Regression | Regression (interpretable units) | Same units as target, sensitive to outliers |\n",
    "| **Binary Cross-Entropy** | `-[y·log(ŷ) + (1-y)·log(1-ŷ)]` | Classification | Binary classification | Probabilistic, penalizes wrong confidence |\n",
    "| **Categorical Cross-Entropy** | `-∑ y·log(ŷ)` | Classification | Multi-class classification | Multi-class generalization, softmax output |\n",
    "\n",
    "### Key Notes:\n",
    "- $\\hat{y}$ is the predicted output from the model\n",
    "- $y$ is the true/actual/desired (target) value (the ground truth from dataset)\n",
    "- $MSE$ = Mean Squared Error, $MAE$ = Mean Absolute Error\n",
    "- $L1/L2$ can refer to either loss functions or regularization terms\n",
    "- **Cross-entropy** losses work with probability outputs (sigmoid/softmax)\n",
    "- Choice depends on problem type, outlier sensitivity, and optimization needs\n",
    "\n",
    "### Formulation\n",
    "Gradient descent is an iterative algorithm in which the key mathematical tool is the **first-order Taylor approximation**.\n",
    "\n",
    "$$w^{t + 1} = w^t - \\eta \\nabla L(w^t)$$\n",
    "$$w^0 = w^\\text{ initial}$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ is the **learning rate**.\n",
    "- $\\nabla$ is the **Laplace operator (Laplacian)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fb4e0-15db-4233-bbe7-b10259c737af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa34f11-a824-47d4-bb0f-5ad169a28adb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] \"User Guide, Scikit-Learn Documentation\", https://scikit-learn.org/stable/user_guide.html, Accessed October 2025.\n",
    "[2] Grus, Joel., \"Data Science from Scratch\", O'Reilly, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6c336-6b99-4d70-a9ef-1291045584dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
